Running on host: tikgpu01
In directory:
Starting on: Sun Oct 16 14:50:08 CEST 2022
SLURM_JOB_ID: 531505
WWWWWWWWW
AAAAAAAAAA
{'exp_dir': 'exp', 'exp_name': 'ego', 'seed': 1234, 'dataset': {'dataset_size': 200, 'max_node_num': 18, 'name': 'ego_18_small'}, 'mcmc': {'eps': [1.0, 0.5, 0.1], 'fixed_node_number': True, 'grad_step_size': [0.05, 0.005, 0.0005], 'name': 'langevin', 'step_num': 1000}, 'model': {'name': 'ppgn', 'models': {'model_1': {'dropout_p': 0.0, 'gnn_hidden_num_list': [16, 16, 16, 16, 16, 16], 'feature_nums': [16, 16, 32, 64, 64, 32, 16], 'channel_num_list': [2, 4, 8, 16, 16, 8, 4], 'name': 'gin', 'use_norm_layers': False}}, 'stack_num': 1}, 'sample': {'batch_size': 32}, 'test': {'batch_size': 32, 'split': 0.2}, 'train': {'batch_size': 32, 'lr_dacey': 0.999, 'lr_init': 0.001, 'momentum': 0.9, 'max_epoch': 5000, 'sample_interval': 1, 'save_interval': 1, 'shuffle': True, 'weight_decay': 0.0}, 'eval_from': 1000, 'samplesize': 1028, 'finalinterval': 10, 'num_layers': 6, 'hidden': 32, 'hidden_final': 32, 'dropout': 1e-06, 'n_nodes': 30, 'normalization': 'instance', 'noise_mlp': False, 'num_levels': [32], 'noisetype': 'switched', 'weighted_loss': True, 'dev': device(type='cuda'), 'run_id': '11067', 'folder_name': 'ppgn_ego_18_small__Oct-16-14-50-13_11067', 'save_dir': 'exp/ego/ppgn_ego_18_small__Oct-16-14-50-13_11067', 'model_save_dir': 'exp/ego/ppgn_ego_18_small__Oct-16-14-50-13_11067/models'}
| 10-16 14:50:13 EXPERIMENT BEGIN: 
| 10-16 14:50:13 logging into exp/ego/ppgn_ego_18_small__Oct-16-14-50-13_11067/info.log
| 10-16 14:50:14 load dataset: ego_small
200
| 10-16 14:50:16 model: Powerful(
  (activation): LeakyReLU(negative_slope=0.01)
  (time_mlp): Sequential(
    (0): Linear(in_features=1, out_features=4, bias=True)
    (1): GELU()
    (2): Linear(in_features=4, out_features=1, bias=True)
  )
  (in_lin): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
  )
  (layer_cat_lin): Sequential(
    (0): Linear(in_features=194, out_features=32, bias=True)
  )
  (convs): ModuleList(
    (0): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (1): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (2): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (3): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (4): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (5): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
  )
  (bns): ModuleList(
    (0): None
    (1): None
    (2): None
    (3): None
    (4): None
    (5): None
  )
  (feature_extractors): ModuleList(
    (0): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (1): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (2): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (3): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (4): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (5): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
  )
  (after_conv): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
  )
  (final_lin): Sequential(
    (0): Linear(in_features=32, out_features=1, bias=True)
  )
)
| 10-16 14:50:16 Parameters: 
time_mlp.0.weight ........................................................................................... torch.Size([4, 1])
time_mlp.0.bias ................................................................................................ torch.Size([4])
time_mlp.2.weight ........................................................................................... torch.Size([1, 4])
time_mlp.2.bias ................................................................................................ torch.Size([1])
in_lin.0.weight ............................................................................................ torch.Size([32, 2])
in_lin.0.bias ................................................................................................. torch.Size([32])
layer_cat_lin.0.weight ................................................................................... torch.Size([32, 194])
layer_cat_lin.0.bias .......................................................................................... torch.Size([32])
convs.0.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.0.m1.0.bias ............................................................................................. torch.Size([32])
convs.0.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.0.m1.2.bias ............................................................................................. torch.Size([32])
convs.0.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.0.m2.0.bias ............................................................................................. torch.Size([32])
convs.0.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.0.m2.2.bias ............................................................................................. torch.Size([32])
convs.0.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.0.m4.0.bias ............................................................................................. torch.Size([32])
convs.1.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.1.m1.0.bias ............................................................................................. torch.Size([32])
convs.1.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.1.m1.2.bias ............................................................................................. torch.Size([32])
convs.1.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.1.m2.0.bias ............................................................................................. torch.Size([32])
convs.1.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.1.m2.2.bias ............................................................................................. torch.Size([32])
convs.1.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.1.m4.0.bias ............................................................................................. torch.Size([32])
convs.2.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.2.m1.0.bias ............................................................................................. torch.Size([32])
convs.2.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.2.m1.2.bias ............................................................................................. torch.Size([32])
convs.2.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.2.m2.0.bias ............................................................................................. torch.Size([32])
convs.2.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.2.m2.2.bias ............................................................................................. torch.Size([32])
convs.2.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.2.m4.0.bias ............................................................................................. torch.Size([32])
convs.3.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.3.m1.0.bias ............................................................................................. torch.Size([32])
convs.3.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.3.m1.2.bias ............................................................................................. torch.Size([32])
convs.3.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.3.m2.0.bias ............................................................................................. torch.Size([32])
convs.3.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.3.m2.2.bias ............................................................................................. torch.Size([32])
convs.3.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.3.m4.0.bias ............................................................................................. torch.Size([32])
convs.4.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.4.m1.0.bias ............................................................................................. torch.Size([32])
convs.4.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.4.m1.2.bias ............................................................................................. torch.Size([32])
convs.4.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.4.m2.0.bias ............................................................................................. torch.Size([32])
convs.4.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.4.m2.2.bias ............................................................................................. torch.Size([32])
convs.4.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.4.m4.0.bias ............................................................................................. torch.Size([32])
convs.5.m1.0.weight ....................................................................................... torch.Size([32, 32])
convs.5.m1.0.bias ............................................................................................. torch.Size([32])
convs.5.m1.2.weight ....................................................................................... torch.Size([32, 32])
convs.5.m1.2.bias ............................................................................................. torch.Size([32])
convs.5.m2.0.weight ....................................................................................... torch.Size([32, 32])
convs.5.m2.0.bias ............................................................................................. torch.Size([32])
convs.5.m2.2.weight ....................................................................................... torch.Size([32, 32])
convs.5.m2.2.bias ............................................................................................. torch.Size([32])
convs.5.m4.0.weight ....................................................................................... torch.Size([32, 64])
convs.5.m4.0.bias ............................................................................................. torch.Size([32])
feature_extractors.0.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.0.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.0.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.0.lin3.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.1.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.1.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.1.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.1.lin3.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.2.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.2.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.2.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.2.lin3.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.3.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.3.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.3.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.3.lin3.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.4.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.4.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.4.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.4.lin3.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.5.lin1.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.5.lin1.0.bias .............................................................................. torch.Size([32])
feature_extractors.5.lin2.0.weight ........................................................................ torch.Size([32, 32])
feature_extractors.5.lin3.0.weight ........................................................................ torch.Size([32, 32])
after_conv.0.weight ....................................................................................... torch.Size([32, 32])
after_conv.0.bias ............................................................................................. torch.Size([32])
final_lin.0.weight ......................................................................................... torch.Size([1, 32])
final_lin.0.bias ............................................................................................... torch.Size([1])
| 10-16 14:50:16 Parameters Count: 63886, Trainable: 63886
| 10-16 14:50:16 Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/usr/itetnas04/data-scratch-01/khaefeli/data/gith/discrete_DPPM_Graphs, universal_newlines=False, shell=None, istream=None)
| 10-16 14:50:16 Starting new HTTPS connection (1): api.wandb.ai:443
| 10-16 14:50:16 https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 521
| 10-16 14:50:16 Starting new HTTPS connection (1): api.wandb.ai:443
| 10-16 14:50:16 https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 329
| 10-16 14:50:16 Starting new HTTPS connection (1): api.wandb.ai:443
| 10-16 14:50:16 https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 521
| 10-16 14:50:16 Starting new HTTPS connection (1): api.wandb.ai:443
| 10-16 14:50:16 https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 329
| 10-16 14:50:17 Popen(['git', 'cat-file', '--batch-check'], cwd=/usr/itetnas04/data-scratch-01/khaefeli/data/gith/discrete_DPPM_Graphs, universal_newlines=False, shell=None, istream=<valid stream>)
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:29 epoch: 000| time: 5.4s| train loss: +4.735e-02 | test loss: +7.217e-02 | 
| 10-16 14:50:29 epoch: 000| train loss i: nan test loss i: nan | 
| 10-16 14:50:29 epoch: 000| time: 5.5s| train loss: +4.735e-02 | test loss: +7.217e-02 | 
| 10-16 14:50:29 epoch: 000| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:34 epoch: 001| time: 5.1s| train loss: +3.965e-02 | test loss: +3.983e-02 | 
| 10-16 14:50:34 epoch: 001| train loss i: nan test loss i: nan | 
| 10-16 14:50:34 epoch: 001| time: 5.1s| train loss: +3.965e-02 | test loss: +3.983e-02 | 
| 10-16 14:50:34 epoch: 001| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:39 epoch: 002| time: 5.0s| train loss: +3.485e-02 | test loss: +3.286e-02 | 
| 10-16 14:50:39 epoch: 002| train loss i: nan test loss i: nan | 
| 10-16 14:50:39 epoch: 002| time: 5.1s| train loss: +3.485e-02 | test loss: +3.286e-02 | 
| 10-16 14:50:39 epoch: 002| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:44 epoch: 003| time: 5.0s| train loss: +3.267e-02 | test loss: +4.783e-02 | 
| 10-16 14:50:44 epoch: 003| train loss i: nan test loss i: nan | 
| 10-16 14:50:44 epoch: 003| time: 5.1s| train loss: +3.267e-02 | test loss: +4.783e-02 | 
| 10-16 14:50:44 epoch: 003| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:49 epoch: 004| time: 5.1s| train loss: +2.841e-02 | test loss: +3.612e-02 | 
| 10-16 14:50:49 epoch: 004| train loss i: nan test loss i: nan | 
| 10-16 14:50:49 epoch: 004| time: 5.1s| train loss: +2.841e-02 | test loss: +3.612e-02 | 
| 10-16 14:50:49 epoch: 004| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:54 epoch: 005| time: 5.1s| train loss: +2.613e-02 | test loss: +3.985e-02 | 
| 10-16 14:50:54 epoch: 005| train loss i: nan test loss i: nan | 
| 10-16 14:50:54 epoch: 005| time: 5.1s| train loss: +2.613e-02 | test loss: +3.985e-02 | 
| 10-16 14:50:54 epoch: 005| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:50:59 epoch: 006| time: 5.0s| train loss: +2.321e-02 | test loss: +2.738e-02 | 
| 10-16 14:50:59 epoch: 006| train loss i: nan test loss i: nan | 
| 10-16 14:50:59 epoch: 006| time: 5.1s| train loss: +2.321e-02 | test loss: +2.738e-02 | 
| 10-16 14:50:59 epoch: 006| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:51:04 epoch: 007| time: 5.0s| train loss: +2.412e-02 | test loss: +3.070e-02 | 
| 10-16 14:51:04 epoch: 007| train loss i: nan test loss i: nan | 
| 10-16 14:51:04 epoch: 007| time: 5.1s| train loss: +2.412e-02 | test loss: +3.070e-02 | 
| 10-16 14:51:04 epoch: 007| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:51:09 epoch: 008| time: 5.0s| train loss: +2.632e-02 | test loss: +2.727e-02 | 
| 10-16 14:51:09 epoch: 008| train loss i: nan test loss i: nan | 
| 10-16 14:51:09 epoch: 008| time: 5.1s| train loss: +2.632e-02 | test loss: +2.727e-02 | 
| 10-16 14:51:09 epoch: 008| train loss i: nan test loss i: nan | 
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
ubu
u
u
u
u
u
u
| 10-16 14:51:14 epoch: 009| time: 5.0s| train loss: +2.507e-02 | test loss: +3.930e-02 | 
| 10-16 14:51:14 epoch: 009| train loss i: nan test loss i: nan | 
| 10-16 14:51:14 epoch: 009| time: 5.0s| train loss: +2.507e-02 | test loss: +3.930e-02 | 
| 10-16 14:51:14 epoch: 009| train loss i: nan test loss i: nan | 
| 10-16 14:51:14 load dataset: ego_small
200
3
[Errno 21] Is a directory: 'exp/ego/ppgn_ego_18_small__Oct-16-14-50-13_11067/models/bestloss'
22
3
2
3
[Errno 21] Is a directory: 'exp/ego/ppgn_ego_18_small__Oct-16-14-50-13_11067/models/best'
22
| 10-16 14:51:15 model: Powerful(
  (activation): LeakyReLU(negative_slope=0.01)
  (time_mlp): Sequential(
    (0): Linear(in_features=1, out_features=4, bias=True)
    (1): GELU()
    (2): Linear(in_features=4, out_features=1, bias=True)
  )
  (in_lin): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
  )
  (layer_cat_lin): Sequential(
    (0): Linear(in_features=194, out_features=32, bias=True)
  )
  (convs): ModuleList(
    (0): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (1): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (2): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (3): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (4): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
    (5): PowerfulLayer(
      (m1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
      (m4): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
      )
    )
  )
  (bns): ModuleList(
    (0): None
    (1): None
    (2): None
    (3): None
    (4): None
    (5): None
  )
  (feature_extractors): ModuleList(
    (0): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (1): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (2): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (3): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (4): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
    (5): FeatureExtractor(
      (lin1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
      )
      (lin2): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (lin3): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=False)
      )
      (activation): LeakyReLU(negative_slope=0.01)
    )
  )
  (after_conv): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
  )
  (final_lin): Sequential(
    (0): Linear(in_features=32, out_features=1, bias=True)
  )
)
