#!/bin/bash -v
#SBATCH --job-name=my_job
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G
#SBATCH --nodes=1
#SBATCH --account=tik-highmem
#SBATCH --gres=gpu:1
#SBATCH --mail-type=ALL                           # mail configuration: NONE, BEGIN, END, FAIL, REQUEUE, ALL
#SBATCH --output=slurm_log/%j.out                 # where to store the output ( %j is the JOBID )
#SBATCH --error=slurm_log/%j.err                  # where to store error messages SBATCH --constraint='tesla_v100|geforce_rtx_3090'

/bin/echo Running on host: `hostname`
/bin/echo In directory: `/itet-stor/khaefeli/net_scratch/khaefeli_graph_ddpm/GraphScoreMatching_ddpm_discrete/GraphScoreMatching_ppgn`
/scratch/slurm/spool/job509634/slurm_script: line 13: /itet-stor/khaefeli/net_scratch/khaefeli_graph_ddpm/GraphScoreMatching_ddpm_discrete/GraphScoreMatching_ppgn: Is a directory
/bin/echo Starting on: `date`
/bin/echo SLURM_JOB_ID: $SLURM_JOB_ID
python3 ppgn_vlb.py -c config/gridsearch/consec_ppgn_1.7_16:47/gridsearch_ppgn_consec_ego_18_small_8,128_1_64_switched_True_1234.yaml
wandb: Currently logged in as: khaefeli (use `wandb login --relogin` to force relogin)
wandb: Appending key for api.wandb.ai to your netrc file: /home/khaefeli/.netrc
wandb: wandb version 0.12.20 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.15
wandb: Run data is saved locally in /usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/wandb/run-20220701_164755-2kpdunb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-cherry-1425
wandb: ‚≠êÔ∏è View project at https://wandb.ai/khaefeli/train_ppgn_consec_gridsearch
wandb: üöÄ View run at https://wandb.ai/khaefeli/train_ppgn_consec_gridsearch/runs/2kpdunb6
/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py:195: DeprecationWarning: This function is deprecated. Please call randint(1, 64 + 1) instead
  sigma_ind_list = np.random.random_integers(low=1,high=config.num_levels[0],size=train_adj_b.size(0))
/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py:248: DeprecationWarning: This function is deprecated. Please call randint(1, 64 + 1) instead
  sigma_ind_list = np.random.random_integers(low=1,high=config.num_levels[0],size=test_adj_b.size(0))
/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Traceback (most recent call last):
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py", line 522, in <module>
    train_main(config_dict, args)
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py", line 503, in train_main
    fit(model, optimizer, None, train_dl,
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py", line 342, in fit
    loss_eval=eval_loss(eval_set,config.num_levels,config,model)
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/ppgn_vlb.py", line 87, in eval_loss
    score_batch=model(A=eval_noise_adj_b_chunked[i].unsqueeze(0).to(config.dev),node_features=eval_noise_adj_b_chunked[i].to(config.dev),mask=mask.to(config.dev),noiselevel=sigma).to(config.dev)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/model/ppgn.py", line 172, in forward
    out = self.forward_cat(A, node_features, mask, noiselevel)
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/model/ppgn.py", line 226, in forward_cat
    u = conv(u, mask) + (u if self.residual else 0)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/itetnas04/data-scratch-01/khaefeli/data/discrete_ddpm_graphs/model/ppgn.py", line 36, in forward
    out2 = self.m2(x).permute(0, 3, 1, 2) * mask           # batch, out_feat, N, N
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/itet-stor/khaefeli/net_scratch/conda_envs/SPECTRE/lib/python3.9/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 1.69 MiB free; 22.22 GiB reserved in total by PyTorch)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: | 2.173 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: / 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: - 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: \ 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: | 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: / 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: - 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: \ 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: | 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: / 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: - 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb: \ 9.492 MB of 9.492 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb: cluster_mmd_64 ‚ñÅ‚ñÅ‚ñÅ
wandb:  degree_mmd_64 ‚ñÅ‚ñÅ‚ñÅ
wandb:       evalloss ‚ñÅ‚ñÅ‚ñÅ
wandb:       testloss ‚ñà‚ñÅ‚ñÖ
wandb:      trainloss ‚ñà‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb: cluster_mmd_64 0
wandb:  degree_mmd_64 0
wandb:       evalloss 0
wandb:       testloss 33.67629
wandb:      trainloss 32.17792
wandb: 
wandb: Synced sandy-cherry-1425: https://wandb.ai/khaefeli/train_ppgn_consec_gridsearch/runs/2kpdunb6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220701_164755-2kpdunb6/logs
exit 0;
